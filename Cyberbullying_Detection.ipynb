{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Cyberbullying Detection Project\n",
    "\n",
    "**Project Overview:**\n",
    "With rise of social media coupled with the Covid-19 pandemic, cyberbullying has reached\r\n",
    "all-time highs. We can combat this by creating models to automatically flag potentially\r\n",
    "harmful tweets as well as break down the patterns of hatre\n",
    "\n",
    "**Project Description:**\n",
    "As social media usage becomes increasingly prevalent in every age group, a vast majority of\r\n",
    "citizens rely on this essential medium for day-to-day communication. Social media’s\r\n",
    "ubiquity means that cyberbullying can effectively impact anyone at any time or anywhere,\r\n",
    "and the relative anonymity of the internet makes such personal attacks more difficult to\r\n",
    "stop than traditional bullying.\r\n",
    "On April 15th, 2020, UNICEF issued a warning in response to the increased risk of\r\n",
    "cyberbullying during the COVID-19 pandemic due to widespread school closures, increased\r\n",
    "screen time, and decreased face-to-face social interaction. The statistics of cyberbullying\r\n",
    "are outright alarming: 36.5% of middle and high school students have felt cyberbullied and\r\n",
    "87% have observed cyberbullying, with effects ranging from decreased academic\r\n",
    "performance to depression to suicidal thoughts.\r\n",
    "In light of all of this, this dataset (i.e., cyberbullying_tweets.csv) contains more than 47,000\r\n",
    "tweets labelled according to the class of cyberbullying:\r\n",
    "• Age;\r\n",
    "• Ethnicity;\r\n",
    "• Gender;\r\n",
    "• Religion;\r\n",
    "• Other type of cyberbullying;\r",
    "• Not cyberbullyingd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cyberbullying_tweets.csv', delimiter=',', encoding='utf-8', header=0)\n",
    "df = df[~df.duplicated()] # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('cyberbullying_type').describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['cyberbullying_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Preprocessing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'rt', '', text, flags=re.IGNORECASE) #remove retweet \n",
    "    text = re.sub(r'http\\S+', '', text)   #remove URLs\n",
    "    text = re.sub(r'@[^\\s]+','',text) #remove users of tweet (@user) not usefull \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)   #remove special characters & punctuation\n",
    "    text = text.lower()   #convert text to lowercase\n",
    "    tokens = word_tokenize(text)   #tokenization\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    #stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    #join tokens back into a single string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "#example usage\n",
    "\n",
    "example_text = df['tweet_text'][100]\n",
    "print(example_text)\n",
    "processed_example = preprocess_text(example_text)\n",
    "print(processed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets are preprocessed and cleaned\n",
    "df['clean_tweet'] = df['tweet_text'].apply(lambda x: preprocess_text(x))\n",
    "# classifications of cyberbullying are assigned a value 0-5\n",
    "df['cyberbullying_type'] = df['cyberbullying_type'].apply(lambda x:  {'not_cyberbullying': 0, 'gender': 1, 'religion': 2, 'age': 3, 'ethnicity': 4, 'other_cyberbullying': 5}[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete long and short tweets\n",
    "\n",
    "df['text_len'] = [len(word_tokenize(tweet)) for tweet in df['clean_tweet']]\n",
    "df = df[(df['text_len'] < 50) & (df['text_len'] > 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"clean_tweet\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Data Split**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# partition X and y\n",
    "X = df['clean_tweet']\n",
    "y = df['cyberbullying_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for testing, training, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2,shuffle = True, random_state = 40)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size =  0.2,shuffle = True, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at if classes are inbalanced\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_valid, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversample Data as classes are inbalanced\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros_train = RandomOverSampler()\n",
    "X_train, y_train = ros_train.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))\n",
    "\n",
    "ros_valid = RandomOverSampler()\n",
    "X_valid, y_valid = ros_valid.fit_resample(np.array(X_valid).reshape(-1,1), np.array(y_valid).reshape(-1,1))\n",
    "\n",
    "train_data = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['clean_tweet', 'cyberbullying_type']);\n",
    "valid_data = pd.DataFrame(list(zip([x[0] for x in X_valid], y_valid)), columns = ['clean_tweet', 'cyberbullying_type']);\n",
    "\n",
    "X_train = train_data[\"clean_tweet\"]\n",
    "y_train = train_data[\"cyberbullying_type\"]\n",
    "\n",
    "X_valid = valid_data[\"clean_tweet\"]\n",
    "y_valid = valid_data[\"cyberbullying_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training, validation, and testing sizes\n",
    "print(\"Training set size: \", len(X_train))\n",
    "print(\"Validation set size: \", len(X_valid))\n",
    "print(\"Testing set size: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### **Model Establishment**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Model: Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "target_names = ['not_cyberbullying', 'gender', 'religion','age','ethnicity','other_cyberbullying']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vect = vectorizer.fit_transform(X_train)\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(X_vect, y_train)\n",
    "\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your training, validation, testing sets into the Pythorch environment.\n",
    "#Transform each text into a matrix (e.g., a sequence of word embeddings) in the PyTorch environment\n",
    "import torch\n",
    "\n",
    "def apply_word_embeddings(tweet, model):\n",
    "    tweet = word_tokenize(tweet)\n",
    "    word_embeddings = [model[word] for word in tweet if word in model]\n",
    "    if len(word_embeddings) == 0: #if no words appear in the word2vec model\n",
    "        return torch.zeros(1, word2vec.vector_size)\n",
    "    else:\n",
    "        word_embeddings = np.array(word_embeddings)\n",
    "        return torch.tensor(word_embeddings)\n",
    "\n",
    "\n",
    "X_train_embed = [apply_word_embeddings(tweet, word2vec) for tweet in X_train]\n",
    "X_test_embed = [apply_word_embeddings(tweet, word2vec) for tweet in X_test]\n",
    "X_valid_embed = [apply_word_embeddings(tweet, word2vec) for tweet in X_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build RNN architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Build RNN architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.RNN = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_state = torch.zeros(self.num_layers, x.size(0),self.hidden_size)\n",
    "        out, _ = self.RNN(x, hidden_state)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Hyperparameters\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 6\n",
    "embedding_size = word2vec.vector_size\n",
    "\n",
    "#Define Model\n",
    "RNN_model = RNN(embedding_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "#Define loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(RNN_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(RNN_model,X_train_embed, y_train, num_epochs): \n",
    "    \n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, labels in zip(X_train_embed,y_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = RNN_model(data.unsqueeze(0))\n",
    "            loss = criterion(outputs, torch.tensor([labels]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "           # Get the predicted class\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += 1\n",
    "            total_correct += (predicted.item() == labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = total_correct / total_samples\n",
    "        train_accuracy.append(accuracy)\n",
    "        train_loss.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Train Acc: {round(accuracy,4)}, Loss: {loss.item()}\")\n",
    "\n",
    "    return (train_accuracy, train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy, train_loss = train(RNN_model, X_train_embed, y_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(RNN_model.state_dict(), 'Basic_RNN_Model3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = RNN(embedding_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "RNN_model.load_state_dict(torch.load('Basic_RNN_Model(lr=0.01).pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(x_test, y_test, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in zip(X_test_embed, y_test):\n",
    "            outputs = RNN_model(data.unsqueeze(0))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_pred.append(predicted)\n",
    "            total += 1\n",
    "            correct += (predicted == labels).item()\n",
    "    accuracy = correct/total\n",
    "    return (y_pred, accuracy)\n",
    "\n",
    "y_pred, accuracy = test_model(X_test_embed, y_test, word2vec)\n",
    "print(\"Testing Accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.Series([tensor.item() for tensor in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics such as accuracy, precision, recall, and F1-score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = y_test\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"Accuracy\", accuracy)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"F1-score\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['not_cyberbullying', 'gender', 'religion','age','ethnicity','other_cyberbullying']\n",
    "cm = classification_report(y_true, y_pred, target_names=target_names, output_dict = True)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(tweet):\n",
    "    RNN_model.eval()\n",
    "    with torch.no_grad():\n",
    "        example_tweet_embedding = apply_word_embeddings(tweet, word2vec)\n",
    "        output = RNN_model(example_tweet_embedding.unsqueeze(0))\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "        predicted_class = predicted_class.item()\n",
    "    print(f\"The model predicts that the example tweet is of class:  {target_names[predicted_class]}\")\n",
    "predict_class(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pd.DataFrame(cm).iloc[:-1,:].T, annot = True,  fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['clean_tweet'])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = 50\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='pre', maxlen=maxlen)\n",
    "X_valid = pad_sequences(X_valid, padding='pre', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='pre', maxlen=maxlen)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid= to_categorical(y_valid)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index)+1, word2vec.vector_size))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#embedding_weights = tf.constant(embedding_matrix)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Create the Adam optimizer with the specified learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim = len(tokenizer.word_index)+1, output_dim = 300, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), input_length=maxlen),\n",
    "    GRU(units = 256),\n",
    "    Dense(units = 64, activation = \"relu\"),\n",
    "    Dense(units = 6, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, verbose=0,validation_data=(X_valid, y_valid), batch_size=16,epochs = 10)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy: \" + str(accuracy))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "print(train_loss)\n",
    "print(val_loss)\n",
    "plt.plot(range(1, 6), train_loss, label = \"Training loss\")\n",
    "plt.plot(range(1, 6), val_loss, label = \"Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Epochs vs Loss\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('BEST_GRU_MODEL.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics such as accuracy, precision, recall, and F1-score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Convert y_test to class labels\n",
    "y_true_classes = y_test.argmax(axis=1)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "precision = precision_score(y_true_classes, y_pred_classes, average='macro')\n",
    "recall = recall_score(y_true_classes, y_pred_classes, average='macro')\n",
    "f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
    "\n",
    "target_names = ['not_cyberbullying', 'gender', 'religion','age','ethnicity','other_cyberbullying']\n",
    "cm = classification_report(y_true_classes, y_pred_classes, target_names=target_names)\n",
    "\n",
    "\n",
    "print(cm)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "values = [accuracy, precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a bar plot\n",
    "sns.barplot(x=metrics, y=values)\n",
    "plt.xlabel('Score')\n",
    "plt.ylim(0,1)\n",
    "plt.title('GRU: Evaluation Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.001, 0.0001, 0.00005]\n",
    "accuracy = [0.8463771481653507, 0.8546214584300975, 0.8500928936367859]\n",
    "\n",
    "sns.lineplot(x = learning_rate, y = accuracy)\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"GRU Learning Rate vs. Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries needed for BERT Transformer\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pretrained BERT Transfomer model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = list(X_train.values)\n",
    "X_test_list = list(X_test.values)\n",
    "X_valid_list = list(X_valid.values)\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train_list, padding = True, truncation = True, max_length = 50)\n",
    "X_valid_tokenized = tokenizer(X_valid_list, padding = True, truncation = True, max_length = 50)\n",
    "X_test_tokenized = tokenizer(X_test_list, padding = True, truncation = True, max_length = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom Dataset for our twitter data\n",
    "import torch\n",
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data = list(y_train.values)\n",
    "y_valid_data = list(y_valid.values)\n",
    "y_test_data = list(y_test.values)\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train_data)\n",
    "val_dataset = Dataset(X_valid_tokenized, y_valid_data)\n",
    "test_dataset = Dataset(X_test_tokenized,y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['not_cyberbullying', 'gender', 'religion','age','ethnicity','other_cyberbullying']\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true = labels, y_pred = pred)\n",
    "    precision = precision_score(y_true = labels, y_pred = pred, average='macro')\n",
    "    recall = recall_score(y_true = labels, y_pred = pred, average='macro')\n",
    "    f1 = f1_score(y_true = labels, y_pred = pred, average='macro')\n",
    "    class_matrix = classification_report(y_true = labels, y_pred = pred, target_names=target_names)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1,\"Classification Matrix\":class_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=1,\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    metric_for_best_model='accuracy',\n",
    "    load_best_model_at_end=True,  # Load the best model based on validation metric\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "# Initialize EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=2)\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[early_stopping],  # Add the early stopping callback\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('BertTransformerModel2 (lr = 2e-5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result = trainer.evaluate(eval_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('BertTransformerModel(lr = 1e-4)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.00002, 0.00005, 0.000006]\n",
    "accuracy = [0.9225499303297724, 0.9311425917324663, 0.832559219693451]\n",
    "\n",
    "sns.lineplot(x = learning_rate, y = accuracy)\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"GRU Learning Rate vs. Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bert_accuracy = result['eval_accuracy']\n",
    "bert_precision = result['eval_precision']\n",
    "bert_recall = result['eval_recall']\n",
    "bert_f1 = result['eval_f1']\n",
    "bert_classification = result['eval_Classification Matrix']\n",
    "\n",
    "print(bert_accuracy)\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
    "values = [bert_accuracy, bert_precision, bert_recall, bert_f1]\n",
    "\n",
    "# Create a bar plot\n",
    "sns.barplot(x=metrics, y=values)\n",
    "plt.xlabel('Score')\n",
    "plt.ylim(0,1)\n",
    "plt.title('BERT: Evaluation Metrics')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Explanation: BERT Shap Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import shap\n",
    "\n",
    "# # load a BERT sentiment analysis model\n",
    "# tokenizer = BertTokenizer.from_pretrained(\n",
    "#     \"BertTransformerModel\"\n",
    "# )\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 6)\n",
    "\n",
    "# define a prediction function\n",
    "def f(x):\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=96, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    )\n",
    "    outputs = model(tv)[0].detach().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    val = sp.special.logit(scores[:, 1])  # use one vs rest logit units\n",
    "    return val\n",
    "\n",
    "\n",
    "# # build an explainer using a token masker\n",
    "explainer = shap.Explainer(f, tokenizer)\n",
    "\n",
    "# # explain the model's predictions \n",
    "shap_values = explainer(X_train[500:510], fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first sentence's explanation\n",
    "shap.plots.text(shap_values[0])\n",
    "\n",
    "#https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.htmlhttps://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values.abs.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
